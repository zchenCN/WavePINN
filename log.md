- Forget optimizer.zero_grad()
- LBFGS perform much better than Adam
- Verify the analytical solution of the PDE
- After modify, refresh the memory of notebook
- Remember loss.item() when append the value of loss in the list
- Training process may slow in the beginning and then accelarate later